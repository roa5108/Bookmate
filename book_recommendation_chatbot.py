import streamlit as st
import openai
import os
import huggingface_hub
from huggingface_hub import login
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import requests
import re
import torch
import time
from bs4 import BeautifulSoup

# ================ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ================
# API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "KEY_OF_OPENAI"
openai.api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY"))
os.environ["NAVER_API_CLIENT_ID"] = "NAVER_API_CLIENT_ID"
os.environ["NAVER_API_CLIENT_SECRET"] = "NAVER_API_CLIENT_SECRET"
HUGGINGFACE_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")

if not HUGGINGFACE_API_TOKEN:
    st.error("Hugging Face API í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    st.stop()

try:
    huggingface_hub.login(HUGGINGFACE_API_TOKEN)
except requests.exceptions.HTTPError as e:
    st.error("Hugging Face ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API í† í°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

# OpenAI Client ìƒì„±
client = openai.OpenAI()

# ì „ì—­ ë³€ìˆ˜ ì„ ì–¸
conversation_text = ""  # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”

# ================ ì „ì—­ ì„¤ì • ë° ëª¨ë¸ ë¡œë“œ ================
# ëª¨ë¸ ë³€ê²½ ë° ì˜ˆì™¸ ì²˜ë¦¬
model_name = "haramjang/gemma-2b-it-chatKeyword"
try:
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
except Exception as e:
    model_name = "google/gemma-2b-it"
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    except Exception as fallback_e:
        st.stop()

def wrapper_generate(tokenizer, model, input_prompt, do_stream=False):
    def get_text_after_prompt(text):
        pattern = r"<start_of_turn>model\s*(.*?)(<end_of_turn>|$)"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            extracted_text = match.group(1).strip()
            return extracted_text
        else:
            return "ë§¤ì¹­ë˜ëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    data = tokenizer(input_prompt, return_tensors="pt")
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    input_ids = data.input_ids[..., :-1]
    with torch.no_grad():
        pred = model.generate(
            input_ids=input_ids.cuda() if torch.cuda.is_available() else input_ids,
            streamer=streamer if do_stream else None,
            use_cache=True,
            max_new_tokens=128,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    decoded_text = tokenizer.batch_decode(pred, skip_special_tokens=False)

    # gemma ê²°ê³¼ì— ëŒ€í•´ íŠ¹ë³„ ì²˜ë¦¬
    return get_text_after_prompt(decoded_text[0])

def function_prepare_sample_text(tokenizer, for_train=True):
    """í´ë¡œì €"""

    def _prepare_sample_text(example):
        """Prepare the text from a sample of the dataset."""
        user_prompt= (
                  "ë‹¤ìŒ ëŒ€í™”ë¥¼ ì½ê³  ì‚¬ìš©ìì˜ ê´€ì‹¬ì‚¬, ì„±í–¥, ê³ ë¯¼ì„ ê°ê° í‚¤ì›Œë“œ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì‹œì˜¤. "
                  "1. ê´€ì‹¬ì‚¬ëŠ” ì‚¬ìš©ìê°€ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ëŠ” ì£¼ì œë‚˜ í™œë™ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
                  "2. ì„±í–¥ì€ ì‚¬ìš©ìì˜ ì„±ê²©ì  íŠ¹ì„±, í–‰ë™ íŒ¨í„´, ë˜ëŠ” ë¬¸ì œë¥¼ ë°”ë¼ë³´ëŠ” íƒœë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
                  "3. ê³ ë¯¼ì€ ì‚¬ìš©ìê°€ ëŒ€í™” ì¤‘ì— í‘œí˜„í•œ ë¬¸ì œë‚˜ ê±±ì •ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
                  "ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì‹­ì‹œì˜¤:\n"
                  "ê´€ì‹¬ì‚¬: [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, ...]\n"
                  "ì„±í–¥: [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, ...]\n"
                  "ê³ ë¯¼: [í‚¤ì›Œë“œ1, í‚¤ì›Œë“œ2, ...]\n"
                  "### ëŒ€í™” ë‚´ìš©: "
              )

        messages = [
            {"role": "user", "content": f"{user_prompt}{example['input']}"},
        ]
        if for_train:
            messages.append({"role": "assistant", "content": f"{example['output']}"})

        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False if for_train else True)
        return text
    return _prepare_sample_text

# ë„¤ì´ë²„ APIë¥¼ ì´ìš©í•œ ë„ì„œ ê²€ìƒ‰
def search_books_naver(client_id, client_secret, keywords):
    url = "https://openapi.naver.com/v1/search/book.json"
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret,
    }
    all_books = []
    for keyword in keywords:
        params = {"query": keyword, "display": 10}
        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()
            data = response.json()
            for item in data.get("items", []):
                title = item.get("title", "").replace("<b>", "").replace("</b>", "")
                author = item.get("author", "")
                description = item.get("description", "").replace("<b>", "").replace("</b>", "")
                all_books.append({"title": title, "author": author, "description": description})
        except Exception as e:
            st.error(f"ë„¤ì´ë²„ API ì˜¤ë¥˜: {e}")
    return all_books

def filter_books(books, keywords):
    scored_books = []
    for book in books:
        relevance_score = sum(kw.lower() in book['title'].lower() or kw.lower() in book['description'].lower() for kw in keywords)
        if relevance_score > 0:
            scored_books.append((book, relevance_score))
    scored_books.sort(key=lambda x: x[1], reverse=True)
    return [book for book, score in scored_books]

# ëŒ€í™” ê¸°ë¡ ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìš”ì¦˜ ì–´ë–»ê²Œ ì§€ë‚´ê³  ê³„ì„¸ìš”?"}]

# ì´ì „ ëŒ€í™” í‘œì‹œ
st.title("ğŸ“š ë¶ë©”ì´íŠ¸ - ì±… ì¶”ì²œ ì±—ë´‡")
st.write("**ë‹¹ì‹ ì˜ ì·¨í–¥ì— ë§ëŠ” ì±…ì„ ì¶”ì²œí•´ë“œë ¤ìš”!** ì €ì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ì„œ ë§ì¶¤í˜• ì±…ì„ ì¶”ì²œí•´ ë“œë¦´ê²Œìš”!")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # ìƒë‹´ì‚¬ ì‘ë‹µ ì²˜ë¦¬
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=st.session_state.messages
        )
        assistant_message = response.choices[0].message.content
    except Exception as e:
        assistant_message = f"ì—ëŸ¬ ë°œìƒ: {e}"

    st.session_state.messages.append({"role": "assistant", "content": assistant_message})
    with st.chat_message("assistant"):
        st.markdown(assistant_message)

# í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì±… ì¶”ì²œ
if st.button("ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ê³  ì±… ì¶”ì²œ ë°›ê¸°"):
    conversation_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.messages])
    keywords_output = wrapper_generate(tokenizer, model, conversation_text)
    st.write("**í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼:**", keywords_output)

    keywords = re.findall(r"\[([^\]]+)\]", keywords_output)
    extracted_keywords = sum((kw.split(',') for kw in keywords), [])
    extracted_keywords = [k.strip() for k in extracted_keywords if k.strip()]

    if extracted_keywords:
        client_id = os.getenv("NAVER_API_CLIENT_ID")
        client_secret = os.getenv("NAVER_API_CLIENT_SECRET")
        books = search_books_naver(client_id, client_secret, extracted_keywords)
        filtered_books = filter_books(books, extracted_keywords)
        st.subheader("ğŸ“– ì¶”ì²œ ë„ì„œ")
        for book in filtered_books[:4]:
            st.write(f"**ì œëª©:** {book['title']}")
            st.write(f"**ì €ì:** {book['author']}")
            st.write(f"**ì„¤ëª…:** {book['description']}")
            st.markdown("---")
    else:
        st.write("ì¶”ì²œí•  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")